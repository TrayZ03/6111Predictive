---
title: "Preliminary Results"
author: "Tracey Zicherman"
date: "2024-04-01"
always_allow_html: true
output: word_document
---

# Setup

In this section we load the necessary libraries, and configure other settings for the remainder of the document.

```{r include=FALSE}
# install libraries - un-comment if needed
# install.packages("tidyverse")
# install.packages("kableExtra")
# install.packages("leaps")
# install.packages("caret")
# install.packages("tidymodels")
# install.packages("recipes")
# install.packages("kknn")
# install.packages("glmnet")
# install.package("gridExtra")
install.packages("officer")
install.packages("officedown")

# load libraries
library(tidyverse)
library(kableExtra)
library(gridExtra)
library(leaps)
library(caret)
library(tidymodels)
library(recipes)
library(kknn)
library(glmnet)
tidymodels_prefer()
```



```{r}
# read .csv in as a tibble
data <- as.tibble(read.csv("customer_data.csv")) # modify local path to file
data
```

# Overview

ABC Corporation is in need of a supervised classification model which will predict the probability of customer attrition based on demographic, behavioral and service-related features. Target (dependent variable) is risk of attrition, represented by a value between 0 and 1 which indicates probability that a customer will terminate services rendered by ABC Corporation.

The expected outcomes of these preliminary results are the following:

1. ***Analytic***: Trained classification model which predicts customer attrition based on selected features with a high degree of accuracy. Determination as to which features are most influential in predicting customer attrition.
2. ***Informational***: Development of insights as to customer behaviors and demographics associated with customer attrition.
3. ***Model Usage***: Prediction of likelihood of customer attrition for both new and existing customers. Those identified as likely of attrition can then be targeted via direct advertising or incentives, etc. so as to prevent said attrition. This will lead to increased customer retention, and subsequently increased revenue for ABC Corporation.

## Data Dictionary

Here we display the data dictionary for this dataset, which includes each variable and a description.

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# read data dictionary into tibble
data_dict <- read_tsv('project_data_dictionary.txt')
data_dict
# # create table with kable, and modify heights - this print nicely and gets all columns on same page
# kable(data_dict, "html") %>%
#   kable_styling(full_width = F, position = "left") %>%
#   column_spec(1, width = "300px") %>%  # Adjust width as needed
#   column_spec(2, width = "300px")      # Adjust width as needed
```

Client Number is a unique identifier, and thus isn't useful as predictor of probability of attrition, we will drop this column before modeling. 

## Classification Task

First, let us consider the overall classification task - namely, to predict customer attrition. To that end, let us inspect the distribution of attrited vs non-attrited ("existing") customers.

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# create summary tibble with proportions of each Attrition_Flag value
attr_dist <- data %>%
  group_by(Attrition_Flag) %>%
  summarize(Count=n()) %>%
  mutate(Proportion=round(Count/sum(Count), 2))

# create the bar plot
plot <- ggplot(attr_dist, aes(x = Attrition_Flag, y = Proportion, fill = Attrition_Flag)) +
  geom_bar(stat = "identity") +
  labs(title = "Distribution of Attrition_Flag",
       x = "Category",
       y = "Proportion") +
  scale_fill_manual(values = c("Existing Customer" = "darkblue", "Attrited Customer" = "darkgreen"))

plot
```

# Data Processing

In this section we perform any steps necessary to prepare the data

## Remove Problematic Columns or Rows

Early investigation determined that errors and duplicates did not appear to be an issue, so we only need to remove the `CLIENTNUM` column.

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# remove first column
cleaned_data <-data[, -1 ]
cleaned_data
```

## Check Data Types

Inspecting the data types of the columns in the tibble above, that were inferred when the .csv was read, everything looks correct. The categorical variables were read in as string which makes sense, however we will convert the character (string) type columns to factor type (R's native categorical data type), for better compatibility with other functions, features, etc.

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# mutate each chr categorical to fct
data <- data %>%
  mutate(Attrition_Flag = as.factor(Attrition_Flag),
         Gender = as.factor(Gender),
         Education_Level = as.factor(Education_Level),
         Marital_Status = as.factor(Marital_Status),
         Income_Category = as.factor(Income_Category),
         Card_Category = as.factor(Card_Category)
         )

# glimpse to ensure types look good
glimpse(data)
```

## Missing Values

Preliminary EDA showed that missing values are in form of “Unknown,” which occurred in the columns `Education_Level, Marital_Status_Income, Income_Category`, all of which are categorical variables.

The total number of rows which contained the value "Unknown" in at least one of these columns was around 3,000, or approximately 30% of the data, the decision was made to leave "Unknown" present as an additional category in these columns, leading to the creation of an additional binary variable during one-hot encoding.

## Standardizing

As we will be fitting a k-nearest neighbors model, standardizing (namely, centering and scaling )is very important, since the learning algorithm is based on distances between points in a high dimensional space, it is important that each dimension (i.e. axis) is measured on the same scale. 

To make comparison between the k-nearest neighbors model and any other models meaningful, we will standardize the entire data set. Note that this will be accomplished in a recipe in the [Preprocessing](#preproccessing) section below.

# Feature Engineering

Here we create or discuss the later creation of new variables, known as feature engineering. 

## One Hot Encoding 

All categorical variables will be converted to numerical binary variable using one-hot encoding. We have stored all such variables as `fct` (factor) type, here are the names

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# print factor columns
colnames(data %>%
  select(where(is.factor)))
```

<!-- ***SIDEKICK: Note: This was a good idea, but time didn't really permit. I don't think it's essential to do well on the project?*** -->

<!-- ## Binning -->

<!-- This involves using bins to delineate between ranges would be useful for variable such as Customer Age as most obvious choice. Income Category already implements this method.  Others may include Months on Book, Credit Limit, Total Revolving Balance, Total Transaction Count, etc., but more analysis will need to be performed to ascertain whether certain variables are more useful for analysis and interpretation when binned. -->


## New Features: 

Here we create some new columns which are ratios of previous columns. The particular choices are guided by intuition for relevant new features describing customer behavior which is relevant to attrition.

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# average transaction
cleaned_data$Av_Trans <- cleaned_data$Total_Trans_Amt / cleaned_data$Total_Trans_Ct

# average monthly spending
cleaned_data$Av_Mon_Spend <- cleaned_data$Total_Trans_Amt / cleaned_data$Months_on_book

# average monthly transactions
cleaned_data$Trans_Freqency <- cleaned_data$Total_Trans_Ct / cleaned_data$Months_on_book

# average credit utilization ratio
cleaned_data$Credit_Use_Ratio <- cleaned_data$Total_Revolving_Bal / cleaned_data$Credit_Limit
```


# Preprocessing

Here we prepare cleaned data for modeling

## Train Test Split

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# Perform train test split
data_split <- initial_split(cleaned_data, strata = "Attrition_Flag", prop = 0.75)
train_set <- training(data_split)
test_set  <- testing(data_split)

# inspect dimension of split sets
cat("Train set dimensions:", dim(train_set), "\n")
cat("Test set dimensions:", dim(test_set))
```

## Preprocessing Recipe

We perform the following preprocessing steps

1. `step_other` - this pools infrequently occurring values of categorical variables into another category called `"other"`
2. `step_dummy` - this one-hot encodes all categorical variables. 
3. `step_center` - this centers all variables (which are all numeric by this stage in the processing pipeline) by subtracting them from their mean.
4. `step_scale` - this scales all variables by dividing them by their standard deviation.

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# create preprocessing recipt
rec <- recipe(Attrition_Flag ~ ., data = train_set) %>%
  step_other(Education_Level, Marital_Status, Income_Category, Card_Category) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

# Hyperparameter Tuning with Cross-Validation on Train Data

In this section we tune the hyperparameters classifier models, logistic regression (a parametric model) and k-nearest neighbors (a non-parametric model). 

We use 10-fold cross-validation step when fitting the model to the training data, to find improved hyperparameters for each, namely the regularization penalty for logistic regression, and the number of neighbors for k-nearest neighbors. Optimization is with respect to the area under the receiver operating characteristic curve (ROC-AUC), a good metric for a balanced classifier with an 

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# create models
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

knn_mod <- 
  nearest_neighbor(neighbors = tune()) %>% 
  set_mode("classification") %>%
  set_engine("kknn")

# split the training set into 10 cv folds
cv_set <- vfold_cv(train_set, strata = "Attrition_Flag", v = 10)

# create workflows
lr_workflow <- 
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(lr_mod)

knn_workflow <- 
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(knn_mod)

# create range of values for to try during tuning
lr_reg_grid <- tibble(penalty = 10^seq(-4, -1, length.out = 30))
knn_neighbors_grid <- tibble(neighbors = 3:10)

# store results of cv tuning
lr_results <- 
  lr_workflow %>% 
  tune_grid(cv_set,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

knn_results <- 
  knn_workflow %>% 
  tune_grid(cv_set,
            grid = knn_neighbors_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

Here is a plot of the hyperparameter values versus the mean ROC-AUC score over all 10 cross-validation folds.

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# create separate plots for both models
lr_plot <- 
  lr_results %>% 
  collect_metrics() %>% 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  scale_x_log10(labels = scales::label_number()) +
  labs(
    title = "Hyperparameter Tuning Results (Logistic Regression)",
    subtitle = "",
    caption = "",
    x = "10-Fold Cross-Validation (Mean) ROC AUC",
    y = "Regularization Penalty"
  )

knn_plot <- 
  knn_results %>% 
  collect_metrics() %>% 
  ggplot(aes(x = neighbors, y = mean)) + 
  geom_point() + 
  geom_line() + 
  scale_x_log10(labels = scales::label_number()) +
  labs(
    title = "Hyperparameter Tuning Results (K-Nearest Neighbors)",
    subtitle = "",
    caption = "",
    x = "10-Fold Cross-Validation (Mean) ROC AUC",
    y = "K (Neighbors)"
  )

# display plots in a grid
grid.arrange(lr_plot, knn_plot, nrow = 2, heights = c(1, 1))
```

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# create tibble of results for top 5 models
top_lr_models <-
  lr_results %>% 
  show_best(metric = "roc_auc", n = 5) %>% 
  rename(Mean_CV_ROC_AUC=mean, Std_Err_CV_ROC_AUC=std_err, Hyperparameter_Value=penalty) %>%
  mutate(Hyperparameter="Penalty", Model="Logistic Regression") %>%
  select(Model, Hyperparameter, Hyperparameter_Value, Mean_CV_ROC_AUC, Std_Err_CV_ROC_AUC)

top_knn_models <-
  knn_results %>% 
  show_best(metric = "roc_auc", n = 5) %>%
  rename(Mean_CV_ROC_AUC=mean, Std_Err_CV_ROC_AUC=std_err, Hyperparameter_Value=neighbors) %>%
  mutate(Hyperparameter="Neighbors", Model="K-Nearest Neighbors") %>%
  select(Model, Hyperparameter, Hyperparameter_Value, Mean_CV_ROC_AUC, Std_Err_CV_ROC_AUC)

top_models <- rbind(top_lr_models, top_knn_models)

# print result tibble
print(top_models)
```

# Evaluate Tuned Models

In this section we fit final tuned models using the best hyperparameters obtained from cross-validation in the last section. 

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# get best hyperparameter values from tuning results
best_lr_penalty <- top_models %>%
  filter(Model == "Logistic Regression") %>%
  filter(Mean_CV_ROC_AUC == max(Mean_CV_ROC_AUC)) %>%
  select(Hyperparameter_Value)

best_knn_neighbors <- top_models %>%
  filter(Model == "K-Nearest Neighbors") %>%
  filter(Mean_CV_ROC_AUC == max(Mean_CV_ROC_AUC)) %>%
  select(Hyperparameter_Value)

# create models using best hyperparameter values
best_lr_mod <- 
  logistic_reg(penalty = best_lr_penalty, mixture = 1) %>% 
  set_engine("glmnet")

best_knn_mod <- 
  nearest_neighbor(neighbors = best_knn_neighbors) %>% 
  set_mode("classification") %>%
  set_engine("kknn")

# prepare training data in the recipe
rec <- rec %>%
  prep(training = train_set)

# fit models
best_lr_fit <- 
  best_lr_mod %>%
  set_engine("glm") %>%
  fit(Attrition_Flag ~ ., data = bake(rec, new_data = train_set))

best_knn_fit <- 
  best_knn_mod %>%
  set_engine("kknn") %>%
  fit(Attrition_Flag ~ ., data = bake(rec, new_data = train_set))
```


## Collect Predictions

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# bake testing data with the recipe
test_baked <- bake(rec, new_data = test_set, all_predictors())

# combine ground truth class labels with predicted probabilities for logistic regression
lr_test_results <- 
  test_set %>%
  select(Attrition_Flag) %>%
  bind_cols(
    predict(best_lr_fit, new_data = test_baked, type = "prob")
  ) 

# get rid of problematic characters in column names
colnames(lr_test_results) <- make.names(colnames(lr_test_results))

# inspect results
lr_test_results
```

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# combine ground truth class labels with predicted probabilities for k-nearest neighbors
knn_test_results <- 
  test_set %>%
  select(Attrition_Flag) %>%
  bind_cols(
    predict(best_knn_fit, new_data = test_baked, type = "prob")
  )

# get rid of problematic characters in column names
colnames(knn_test_results) <- make.names(colnames(knn_test_results))

# inspect results
knn_test_results
```

## Classifier Metrics

For this classification problem the classes are relatively unbalanced. For this reason, a good choice (and indeed, the standard choice) for the null model is the model which predicts the mode (most frequently occurring class), name `"Existing Customer"`, in other words, predicting no customer attrition (this makes sense, as it is a relatively rare event)

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# get ground truth class labels for null model
null_test_results <- 
  test_set %>%
  select(Attrition_Flag)

# predict no customer
null_test_results$.pred_Attrited.Customer <- 0
null_test_results$.pred_Existing.Customer <- 1

null_test_results
```

### Accuracy

In this section we report the overall accuracy for the test data.


```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# create tibble with predictions from both models for probability threshold of 0.5
preds <- tibble(True=ifelse(null_test_results$Attrition_Flag == "Attrited Customer", 1, 0),
                Null=null_test_results$.pred_Attrited.Customer,
                LR=as.integer(lr_test_results$.pred_Attrited.Customer >= 0.5),
                KNN=as.integer(knn_test_results$.pred_Attrited.Customer >= 0.5))

null_acc <- sum(preds$Null == preds$True)/dim(preds)[1]
lr_acc <- sum(preds$LR == preds$True)/dim(preds)[1]
knn_acc <- sum(preds$KNN == preds$True)/dim(preds)[1]
acc_results <- tibble(Model=c("Null Model", "Logistic Regression", "K-Nearest Neighbors"),
                  accuracy=c(null_acc, lr_acc, knn_acc))
acc_results
```

In terms of overall accuracy, logistic regression is the clear winner. 

Note that the accuracy of the null model is relatively high, reflecting the imbalance of the classes, that is, the dominance of the existing customers in the data set. Note the proportion of the existing (non-attrited) customers in the data set is exactly the null model accuracy.

### ROC Curves

```{r echo=FALSE, warnings=FALSE, messages=FALSE}
# create data frames to store ROC AUC calculations for each probability threshold
roc_data <- data.frame(threshold=seq(1,0,-0.01), lr_fpr=0, knn_tpr=0)

for (i in roc_data$threshold) {
  # subset the test result based on which predictions are over the threshold i
  lr_over_threshold <- lr_test_results[lr_test_results$.pred_Attrited.Customer >= i, ]
  knn_over_threshold <- knn_test_results[knn_test_results$.pred_Attrited.Customer >= i, ]
 
  # get the predicted negative class for each model
  lr_pred_neg <- sum(lr_over_threshold$Attrition_Flag=="Existing Customer")
  knn_pred_neg <- sum(knn_over_threshold$Attrition_Flag=="Existing Customer")
  
  # get the predicted positive class for each model
  lr_pred_pos <- sum(lr_over_threshold$Attrition_Flag=="Attrited Customer")
  knn_pred_pos <- sum(knn_over_threshold$Attrition_Flag=="Attrited Customer")
  
  # get the true positives and negatives (same for each model)
  true_neg <- sum(lr_test_results$Attrition_Flag=="Existing Customer")
  true_pos <- sum(lr_test_results$Attrition_Flag=="Attrited Customer")
  
  # compute false and true positive rates and store in roc_data tibble
  lr_fpr <- lr_pred_neg/true_neg
  knn_fpr <- knn_pred_neg/true_neg
  roc_data[roc_data$threshold==i, "lr_fpr"] <- lr_fpr
  roc_data[roc_data$threshold==i, "knn_fpr"] <- knn_fpr
  
  lr_tpr <- lr_pred_pos/true_pos
  knn_tpr <- knn_pred_pos/true_pos
  roc_data[roc_data$threshold==i, "lr_tpr"] <- lr_tpr
  roc_data[roc_data$threshold==i, "knn_tpr"] <- knn_tpr
  
}
```

```{r, echo=FALSE, warnings=FALSE, messages=FALSE, fig.height=10, fig.width=8}

# create custom color palettes
blue_palette <- colorRampPalette(c("lightblue", "blue", "darkblue"))
green_palette <- colorRampPalette(c("lightgreen", "green", "darkgreen"))

# create three color palette for plot
blues <- blue_palette(3)
greens <- green_palette(3)

lr_plot <- ggplot() +
  geom_line(data = roc_data, aes(x = lr_fpr, y = lr_tpr, color = threshold), linewidth = 2) +
  scale_color_gradientn(colors=blues) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_point(data = roc_data[seq(1, 101, 10), ], aes(x = lr_fpr, y = lr_tpr)) +
  geom_text(data = roc_data[seq(1, 101, 10), ],
            aes(x = lr_fpr, y = lr_tpr, label = threshold, hjust = 1.2, vjust = -0.2)) +
  labs(
    title = "ROC Curve (Logistic Regression)",
    subtitle = "",
    caption = "",
    x = "False Positive Rate (Positive Class: 'Attrited Customer')",
    y = "True Positive Rate"
  )

knn_plot <- ggplot() +
  geom_line(data = roc_data, aes(x = knn_fpr, y = knn_tpr, color = threshold), linewidth = 2) +
  scale_color_gradientn(colors=greens) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_point(data = roc_data[seq(1, 101, 10), ], aes(x = knn_fpr, y = knn_tpr)) +
  geom_text(data = roc_data[seq(1, 101, 10), ],
            aes(x = knn_fpr, y = knn_tpr, label = threshold, hjust = 1.2, vjust = -0.2)) + 
  labs(
    title = "ROC Curve (K-Nearest Neighbors)",
    subtitle = "",
    caption = "",
    x = "False Positive Rate (Positive Class: 'Attrited Customer')",
    y = "True Positive Rate"
  )

grid.arrange(lr_plot, knn_plot, nrow=2)
```

From these plots, we can see the tuned logistic regression classifier has a better area under the curve than the k-nearest neighbors classifier, as expected, since the logistic regression mean ROC AUC found during cross-validation was higher.

This shows that logistic regression is better at predicting the positive class at all probability thresholds, which indicates it is overall better suited to this data set as a predictive model.

### Calibration Curves

```{r, echo=FALSE, warnings=FALSE, messages=FALSE, fig.height=4, fig.width=8}
# initialize data frames for tracking calculation
lr_calibration_data <- data.frame(bin_midpoint=seq(0.05,0.95,0.1),
                                  observed_event_percentage=0,
                                  classifier="LR")
knn_calibration_data <- data.frame(bin_midpoint=seq(0.05,0.95,0.1),
                                   observed_event_percentage=0,
                                   classifier="KNN")

for (i in seq(0.05,0.95,0.1)) {
  
  # For linear regression
  lr_in_interval <- lr_test_results[lr_test_results$.pred_Attrited.Customer >= (i-0.05) & lr_test_results$.pred_Attrited.Customer <= (i+0.05), ]
  lr_oep <- round(nrow(lr_in_interval[lr_in_interval$Attrition_Flag=="Attrited Customer", ])/nrow(lr_in_interval), 2)
  lr_calibration_data[lr_calibration_data$bin_midpoint==i, "observed_event_percentage"] <- lr_oep
  
  # For k-nearest neighbors
  knn_in_interval <- knn_test_results[knn_test_results$.pred_Attrited.Customer >= (i-0.05) & knn_test_results$.pred_Attrited.Customer <= (i+0.05), ]
  knn_oep <- round(nrow(knn_in_interval[knn_in_interval$Attrition_Flag=="Attrited Customer", ])/nrow(knn_in_interval), 2)
  knn_calibration_data[knn_calibration_data$bin_midpoint==i, "observed_event_percentage"] <- knn_oep
}

ggplot() +
  geom_line(data = lr_calibration_data, 
            aes(x = bin_midpoint, y = observed_event_percentage, color= classifier)
            ) +
  geom_line(data = knn_calibration_data, 
            aes(x = bin_midpoint, y = observed_event_percentage, color= classifier)
            ) +
  geom_point(data = lr_calibration_data, 
            aes(x = bin_midpoint, y = observed_event_percentage, color= classifier), 
            size = 2) +
  geom_point(data = knn_calibration_data, 
            aes(x = bin_midpoint, y = observed_event_percentage, color= classifier), 
            size = 2) +
  geom_text(data = lr_calibration_data, 
            aes(x = bin_midpoint, y = observed_event_percentage, label = observed_event_percentage),
            hjust = 0.75, vjust = -0.5, color = "darkblue", size=3) +  
  geom_text(data = knn_calibration_data, 
            aes(x = bin_midpoint, y = observed_event_percentage, label = observed_event_percentage),
            hjust = 0.75, vjust = -0.5, color = "darkgreen", size=3) + 
  scale_color_manual(values = c("LR" = "darkblue", "KNN" = "darkgreen")) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  scale_x_continuous(breaks = seq(0, 1, 0.2), labels = seq(0, 1, 0.2)) +
  scale_y_continuous(breaks = seq(0, 1, 0.2), labels = seq(0, 1, 0.2)) +
  labs(
    title = "Calibration Curves",
    subtitle = "",
    caption = "",
    x = "Mean Predicted Probability (Positive Class: 'Attrited Customer')",
    y = "Fraction of Positives"
  )
```
Again, we see better results from logistic regression. Recall that the dashed line represents a perfect calibration, where the mean predicted probability of the positive class matches the true proportion of the positive class for each probability bin.

In this case, the k-nearest neighbors classifier is clearly regularly overestimating the positive class - for most bins, the fraction of predicted positive class is higher than expected for a perfect fit (where the expected probability is the midpoint of the probability bin) and higher than logistic regression.

# Results

It is pretty clear from these results a logistic regression model will provide a better model overall, both from the perspective of interpretation (since it is a parametric model) and from the perspective of prediction (since it performed better than K-Nearest neighbors on all metrics).

With respect to further pre-processing steps and feature engineering, binning some of the categorical features might be helpful, particularly since there are overall a large number of features. We will explore this in the final report.

We will also explore feature selection techniques in the final report, which will enable us to find further evidence of which features have a strong association with customer attrition, potentially leading to further feature engineering.
















